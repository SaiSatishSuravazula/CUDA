{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOkOMht7l9X3vn8dnFxalS/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Comparing Reductions\n","\n","\n","For your first task, the code is already written for you. We will compare 3 of the reductions given during the presentation: the naive atomic-only reduction, the classical parallel reduction with atomic finish, and the warp shuffle reduction (with atomic finish)."],"metadata":{"id":"3AEPoZCmFwyx"}},{"cell_type":"code","source":["%%writefile reductions.cu\n","\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const size_t N = 8ULL*1024ULL*1024ULL;  // data size\n","//const size_t N = 256*640; // data size\n","const int BLOCK_SIZE = 256;  // CUDA maximum is 1024\n","// naive atomic reduction kernel\n","__global__ void atomic_red(const float *gdata, float *out){\n","  size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","  if (idx < N) atomicAdd(out, gdata[idx]);\n","}\n","\n","__global__ void reduce(float *gdata, float *out){\n","     __shared__ float sdata[BLOCK_SIZE];\n","     int tid = threadIdx.x;\n","     sdata[tid] = 0.0f;\n","     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","\n","     while (idx < N) {  // grid stride loop to load data\n","        sdata[tid] += gdata[idx];\n","        idx += gridDim.x*blockDim.x;\n","        }\n","\n","     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n","        __syncthreads();\n","        if (tid < s)  // parallel sweep reduction\n","            sdata[tid] += sdata[tid + s];\n","        }\n","     if (tid == 0) out[blockIdx.x] = sdata[0];\n","  }\n","\n"," __global__ void reduce_a(float *gdata, float *out){\n","     __shared__ float sdata[BLOCK_SIZE];\n","     int tid = threadIdx.x;\n","     sdata[tid] = 0.0f;\n","     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","\n","     while (idx < N) {  // grid stride loop to load data\n","        sdata[tid] += gdata[idx];\n","        idx += gridDim.x*blockDim.x;\n","        }\n","\n","     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n","        __syncthreads();\n","        if (tid < s)  // parallel sweep reduction\n","            sdata[tid] += sdata[tid + s];\n","        }\n","     if (tid == 0) atomicAdd(out, sdata[0]);\n","  }\n","\n","\n","__global__ void reduce_ws(float *gdata, float *out){\n","     __shared__ float sdata[32];\n","     int tid = threadIdx.x;\n","     int idx = threadIdx.x+blockDim.x*blockIdx.x;\n","     float val = 0.0f;\n","     unsigned mask = 0xFFFFFFFFU;\n","     int lane = threadIdx.x % warpSize;\n","     int warpID = threadIdx.x / warpSize;\n","     while (idx < N) {  // grid stride loop to load\n","        val += gdata[idx];\n","        idx += gridDim.x*blockDim.x;\n","        }\n","\n"," // 1st warp-shuffle reduction\n","    for (int offset = warpSize/2; offset > 0; offset >>= 1)\n","       val += __shfl_down_sync(mask, val, offset);\n","    if (lane == 0) sdata[warpID] = val;\n","   __syncthreads(); // put warp results in shared mem\n","\n","// hereafter, just warp 0\n","    if (warpID == 0){\n"," // reload val from shared mem if warp existed\n","       val = (tid < blockDim.x/warpSize)?sdata[lane]:0;\n","\n"," // final warp-shuffle reduction\n","       for (int offset = warpSize/2; offset > 0; offset >>= 1)\n","          val += __shfl_down_sync(mask, val, offset);\n","\n","       if  (tid == 0) atomicAdd(out, val);\n","     }\n","  }\n","\n","\n","\n","\n","int main(){\n","\n","  float *h_A, *h_sum, *d_A, *d_sum;\n","  h_A = new float[N];  // allocate space for data in host memory\n","  h_sum = new float;\n","  for (int i = 0; i < N; i++)  // initialize matrix in host memory\n","    h_A[i] = 1.0f;\n","  cudaMalloc(&d_A, N*sizeof(float));  // allocate device space for A\n","  cudaMalloc(&d_sum, sizeof(float));  // allocate device space for sum\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy matrix A to device:\n","  cudaMemcpy(d_A, h_A, N*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  cudaMemset(d_sum, 0, sizeof(float));\n","  cudaCheckErrors(\"cudaMemset failure\");\n","  //cuda processing sequence step 1 is complete\n","  atomic_red<<<(N+BLOCK_SIZE-1)/BLOCK_SIZE, BLOCK_SIZE>>>(d_A, d_sum);\n","  cudaCheckErrors(\"atomic reduction kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector sums from device to host:\n","  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"atomic reduction kernel execution failure or cudaMemcpy H2D failure\");\n","  if (*h_sum != (float)N) {printf(\"atomic sum reduction incorrect!\\n\"); return -1;}\n","  printf(\"atomic sum reduction correct!\\n\");\n","  const int blocks = 640;\n","  cudaMemset(d_sum, 0, sizeof(float));\n","  cudaCheckErrors(\"cudaMemset failure\");\n","  //cuda processing sequence step 1 is complete\n","  reduce_a<<<blocks, BLOCK_SIZE>>>(d_A, d_sum);\n","  cudaCheckErrors(\"reduction w/atomic kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector sums from device to host:\n","  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy H2D failure\");\n","  if (*h_sum != (float)N) {printf(\"reduction w/atomic sum incorrect!\\n\"); return -1;}\n","  printf(\"reduction w/atomic sum correct!\\n\");\n","  cudaMemset(d_sum, 0, sizeof(float));\n","  cudaCheckErrors(\"cudaMemset failure\");\n","  //cuda processing sequence step 1 is complete\n","  reduce_ws<<<blocks, BLOCK_SIZE>>>(d_A, d_sum);\n","  cudaCheckErrors(\"reduction warp shuffle kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector sums from device to host:\n","  cudaMemcpy(h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"reduction warp shuffle kernel execution failure or cudaMemcpy H2D failure\");\n","  if (*h_sum != (float)N) {printf(\"reduction warp shuffle sum incorrect!\\n\"); return -1;}\n","  printf(\"reduction warp shuffle sum correct!\\n\");\n","  return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OJuUYNrF8mD","executionInfo":{"status":"ok","timestamp":1741334918908,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"430f8c0f-b469-478c-f675-5542e20d4304"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing reductions.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 reductions.cu -o reductions\n","!./reductions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s51gQ4wKppc4","executionInfo":{"status":"ok","timestamp":1741334971187,"user_tz":-330,"elapsed":3589,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"76262583-b89f-4476-952e-9426ebdd7183"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["atomic sum reduction correct!\n","reduction w/atomic sum correct!\n","reduction warp shuffle sum correct!\n"]}]},{"cell_type":"markdown","source":["##5.1 Max-finding reduction\n","\n","For this exercise, you are given a fully-functional sum-reduction code, similar to the code used for exercise 1 above, except that we will use the 2-stage reduction method without atomic finish. If you wish you can compile and run it as-is to see how it works. Your task is to modify it (only the kernel) so that it creates a proper max-finding reduction. That means that the kernel should report the maximum value in the data set, rather than the sum of the data set. You are expected to use a similar parallel-sweep-reduction technique. If you need help, refer to the solution."],"metadata":{"id":"VbwSY7mbEYjN"}},{"cell_type":"markdown","source":["###max_reduction.cu\n","\n","```\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const size_t N = 8ULL*1024ULL*1024ULL;  // data size\n","const int BLOCK_SIZE = 256;  // CUDA maximum is 1024\n","\n","__global__ void reduce(float *gdata, float *out, size_t n){\n","     __shared__ float sdata[BLOCK_SIZE];\n","     int tid = threadIdx.x;\n","     sdata[tid] = 0.0f;\n","     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","\n","     while (idx < n) {  // grid stride loop to load data\n","        sdata[tid] += gdata[idx];\n","        idx += gridDim.x*blockDim.x;  \n","        }\n","\n","     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n","        __syncthreads();\n","        if (tid < s)  // parallel sweep reduction\n","            sdata[tid] += sdata[tid + s];\n","        }\n","     if (tid == 0) out[blockIdx.x] = sdata[0];\n","  }\n","\n","int main(){\n","\n","  float *h_A, *h_sum, *d_A, *d_sums;\n","  const int blocks = 640;\n","  h_A = new float[N];  // allocate space for data in host memory\n","  h_sum = new float;\n","  float max_val = 5.0f;\n","  for (size_t i = 0; i < N; i++)  // initialize matrix in host memory\n","    h_A[i] = 1.0f;\n","  h_A[100] = max_val;\n","  cudaMalloc(&d_A, N*sizeof(float));  // allocate device space for A\n","  cudaMalloc(&d_sums, blocks*sizeof(float));  // allocate device space for partial sums\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy matrix A to device:\n","  cudaMemcpy(d_A, h_A, N*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  reduce<<<blocks, BLOCK_SIZE>>>(d_A, d_sums, N); // reduce stage 1\n","  cudaCheckErrors(\"reduction kernel launch failure\");\n","  reduce<<<1, BLOCK_SIZE>>>(d_sums, d_A, blocks); // reduce stage 2\n","  cudaCheckErrors(\"reduction kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector sums from device to host:\n","  cudaMemcpy(h_sum, d_A, sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy D2H failure\");\n","  printf(\"reduction output: %f, expected sum reduction output: %f, expected max reduction output: %f\\n\", *h_sum, (float)((N-1)+max_val), max_val);\n","  return 0;\n","}\n","```"],"metadata":{"id":"oefCQH_0Eur4"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzBmZ314Ddhm","executionInfo":{"status":"ok","timestamp":1741337231067,"user_tz":-330,"elapsed":644,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"37b6d4bd-56b3-45bf-9bfa-852164f8c145"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing max_reduction.cu\n"]}],"source":["%%writefile max_reduction.cu\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const size_t N = 8ULL*1024ULL*1024ULL;  // data size\n","const int BLOCK_SIZE = 256;  // CUDA maximum is 1024\n","\n","__global__ void reduce(float *gdata, float *out, size_t n){\n","     __shared__ float sdata[BLOCK_SIZE];\n","     int tid = threadIdx.x;\n","     sdata[tid] = 0.0f;\n","     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","\n","     while (idx < n) {  // grid stride loop to load data\n","        sdata[tid] = max(sdata[tid],gdata[idx]);\n","        idx += gridDim.x*blockDim.x;\n","        }\n","\n","     for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n","        __syncthreads();\n","        if (tid < s)  // parallel sweep reduction\n","            sdata[tid] = max(sdata[tid], sdata[tid + s]);\n","        }\n","     if (tid == 0) out[blockIdx.x] = sdata[0];\n","  }\n","\n","int main(){\n","\n","  float *h_A, *h_sum, *d_A, *d_sums;\n","  const int blocks = 640;\n","  h_A = new float[N];  // allocate space for data in host memory\n","  h_sum = new float;\n","  float max_val = 5.0f;\n","  for (size_t i = 0; i < N; i++)  // initialize matrix in host memory\n","    h_A[i] = 1.0f;\n","  h_A[100] = max_val;\n","  cudaMalloc(&d_A, N*sizeof(float));  // allocate device space for A\n","  cudaMalloc(&d_sums, blocks*sizeof(float));  // allocate device space for partial sums\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy matrix A to device:\n","  cudaMemcpy(d_A, h_A, N*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  reduce<<<blocks, BLOCK_SIZE>>>(d_A, d_sums, N); // reduce stage 1\n","  cudaCheckErrors(\"reduction kernel launch failure\");\n","  reduce<<<1, BLOCK_SIZE>>>(d_sums, d_A, blocks); // reduce stage 2\n","  cudaCheckErrors(\"reduction kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector sums from device to host:\n","  // We are copying only the first value of d_A, which is the max value. The rest of the values are not relevant.\n","  cudaMemcpy(h_sum, d_A, sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"reduction w/atomic kernel execution failure or cudaMemcpy D2H failure\");\n","  printf(\"reduction output: %f, expected sum reduction output: %f, expected max reduction output: %f\\n\", *h_sum, (float)((N-1)+max_val), max_val);\n","  return 0;\n","}"]},{"cell_type":"code","source":["!nvcc -arch=sm_75 max_reduction.cu -o max_reduction\n","!./max_reduction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRkA-IZXyYvV","executionInfo":{"status":"ok","timestamp":1741337248542,"user_tz":-330,"elapsed":3159,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"06838d63-0a6c-490e-b44c-9bea14070142"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["reduction output: 5.000000, expected sum reduction output: 8388612.000000, expected max reduction output: 5.000000\n"]}]}]}