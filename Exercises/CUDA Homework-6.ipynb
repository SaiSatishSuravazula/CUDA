{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOGIzgtcUzS8mW20mzDAHhz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["These exercises will have you use Unified Memory to utilize GPUs on non-trivial data structures.\n","\n","##6.1 Porting Linked Lists to GPUs\n","\n","For your first task, you are given a code that assembles a linked list on the CPU, and then attempts to print an element from the list. Your task is to modify the code using UM techniques, so that the linked list can be correctly traversed either from CPU code or from GPU code. Hint: there is only one line in the file that needs to be modified to do this exercise.\n","\n","Compile it using the following:\n","\n","The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. nvcc is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n","\n","Correct output should look like this:\n","```\n","key = 3\n","key = 3\n","```\n","If you need help, refer to linked_list_solution.cu"],"metadata":{"id":"l86VYFZ44NdB"}},{"cell_type":"markdown","source":["##linked_list.cu\n","\n","```\n","#include <cstdio>\n","#include <cstdlib>\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","struct list_elem {\n","  int key;\n","  list_elem *next;\n","};\n","\n","template <typename T>\n","void alloc_bytes(T &ptr, size_t num_bytes){\n","\n","  ptr = (T)malloc(num_bytes);\n","}\n","\n","__host__ __device__\n","void print_element(list_elem *list, int ele_num){\n","  list_elem *elem = list;\n","  for (int i = 0; i < ele_num; i++)\n","    elem = elem->next;\n","  printf(\"key = %d\\n\", elem->key);\n","}\n","\n","__global__ void gpu_print_element(list_elem *list, int ele_num){\n","  print_element(list, ele_num);\n","}\n","\n","const int num_elem = 5;\n","const int ele = 3;\n","int main(){\n","\n","  list_elem *list_base, *list;\n","  alloc_bytes(list_base, sizeof(list_elem));\n","  list = list_base;\n","  for (int i = 0; i < num_elem; i++){\n","    list->key = i;\n","    alloc_bytes(list->next, sizeof(list_elem));\n","    list = list->next;}\n","  print_element(list_base, ele);\n","  gpu_print_element<<<1,1>>>(list_base, ele);\n","  cudaDeviceSynchronize();\n","  cudaCheckErrors(\"cuda error!\");\n","}\n","```"],"metadata":{"id":"Ev3hnXCV5RsJ"}},{"cell_type":"markdown","source":["Unified Memory `(cudaMallocManaged)` allows the same memory pointer to be accessed on both host (CPU) and device (GPU) without explicit cudaMemcpy.\n","\n","This function allocates Unified Memory using `cudaMallocManaged()`, meaning ptr can be accessed by both CPU and GPU without manual memory transfers.\n","\n","\n","We dynamically allocate memory for linked list nodes using `cudaMallocManaged()`, making it accessible to both CPU and GPU.\n","Each list_elem has a pointer (next) to the next node in Unified Memory.\n"],"metadata":{"id":"6SroZOLsBVWm"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwT88jtK4Bsi","executionInfo":{"status":"ok","timestamp":1741342068017,"user_tz":-330,"elapsed":44,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"86b097b0-5da0-489b-af3d-78609acebfbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing linked_list_solution.cu\n"]}],"source":["%%writefile linked_list_solution.cu\n","\n","# include <cstdio>\n","# include <cstdlib>\n","// error checking macro\n","# define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","struct list_elem {\n","  int key;\n","  list_elem *next;\n","};\n","\n","template <typename T>\n","void alloc_bytes(T &ptr, size_t num_bytes){\n","\n","  cudaMallocManaged(&ptr, num_bytes);\n","}\n","\n","__host__ __device__\n","void print_element(list_elem *list, int ele_num){\n","  list_elem *elem = list;\n","  for (int i = 0; i < ele_num; i++)\n","    elem = elem->next;\n","  printf(\"key = %d\\n\", elem->key);\n","}\n","\n","__global__ void gpu_print_element(list_elem *list, int ele_num){\n","  print_element(list, ele_num);\n","}\n","\n","const int num_elem = 5;\n","const int ele = 3;\n","int main(){\n","\n","  list_elem *list_base, *list;\n","  alloc_bytes(list_base, sizeof(list_elem));\n","  list = list_base;\n","  for (int i = 0; i < num_elem; i++){\n","    list->key = i;\n","    alloc_bytes(list->next, sizeof(list_elem));\n","    list = list->next;}\n","  print_element(list_base, ele);\n","  gpu_print_element<<<1,1>>>(list_base, ele);\n","  cudaDeviceSynchronize();\n","  cudaCheckErrors(\"cuda error!\");\n","}"]},{"cell_type":"code","source":["! nvcc -arch=sm_75 linked_list_solution.cu -o linked_list_solution\n","! ./linked_list_solution"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFtaJLnZE3Pc","executionInfo":{"status":"ok","timestamp":1741342107963,"user_tz":-330,"elapsed":3459,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"b87c75b0-7255-48b1-f95f-9b4f6a9aac85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["key = 3\n","key = 3\n"]}]},{"cell_type":"markdown","source":["## 6.2 Array Increment\n","\n","In this exercise, you are given a code that increments a large array on the GPU.\n","\n","a. First, compile and profile the code as-is:\n","\n","```\n","nvcc -o array_inc array_inc.cu\n","nsys profile --stats=true ./array_inc\n","```\n","\n","Make a note of the kernel execution duration.\n","\n","b. Now, modify the code to use managed memory. Replace the malloc operations with cudaMallocManaged, and eliminate the cudaMemcpy operations. Do you need to replace the cudaMemcpy operation from device to host with a cudaDeviceSynchronize()? Why? Now, compile and profile the code again. Compare the kernel execution duration to the previous result. Note the profiler indication of CPU and GPU page faults.\n","\n","c. Now, modify the code to insert prefetching of the array to the GPU immediately before the kernel call, and back to the CPU immediately after the kernel call. Compile and profile the code again. Compare the kernel execution time to the previous results. Are there still any page faults? Why?\n","\n","d. Bonus: Modify the code to run the inc() kernel 10000 times in a row instead of just once. What can be said about the impact of memory operations on our runtime? What would this suggest for a real-world application?\n","\n","If you need help, refer to the array_inc_solution.cu."],"metadata":{"id":"bBASp_24FMpz"}},{"cell_type":"markdown","source":["###array_inc.cu\n","\n","```\n","#include <cstdio>\n","#include <cstdlib>\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","template <typename T>\n","void alloc_bytes(T &ptr, size_t num_bytes){\n","\n","  ptr = (T)malloc(num_bytes);\n","}\n","\n","__global__ void inc(int *array, size_t n){\n","  size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","  while (idx < n){\n","    array[idx]++;\n","    idx += blockDim.x*gridDim.x; // grid-stride loop\n","    }\n","}\n","\n","const size_t  ds = 32ULL*1024ULL*1024ULL;\n","\n","int main(){\n","\n","  int *h_array, *d_array;\n","  alloc_bytes(h_array, ds*sizeof(h_array[0]));\n","  cudaMalloc(&d_array, ds*sizeof(d_array[0]));\n","  cudaCheckErrors(\"cudaMalloc Error\");\n","  memset(h_array, 0, ds*sizeof(h_array[0]));\n","  cudaMemcpy(d_array, h_array, ds*sizeof(h_array[0]), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H->D Error\");\n","  inc<<<256, 256>>>(d_array, ds);\n","  cudaCheckErrors(\"kernel launch error\");\n","  cudaMemcpy(h_array, d_array, ds*sizeof(h_array[0]), cudaMemcpyDeviceToHost);\n","  cudaCheckErrors(\"kernel execution or cudaMemcpy D->H Error\");\n","  for (int i = 0; i < ds; i++)\n","    if (h_array[i] != 1) {printf(\"mismatch at %d, was: %d, expected: %d\\n\", i, h_array[i], 1); return -1;}\n","  printf(\"success!\\n\");\n","  return 0;\n","}\n","```"],"metadata":{"id":"IRxmfMrdF468"}},{"cell_type":"code","source":["%%writefile array_inc.cu\n","#include <cstdio>\n","#include <cstdlib>\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","template <typename T>\n","void alloc_bytes(T &ptr, size_t num_bytes){\n","\n","  cudaMallocManaged(&ptr, num_bytes);\n","}\n","\n","__global__ void inc(int *array, size_t n){\n","  size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n","  while (idx < n){\n","    array[idx]++;\n","    idx += blockDim.x*gridDim.x; // grid-stride loop\n","    }\n","}\n","\n","const size_t  ds = 32ULL*1024ULL*1024ULL;\n","\n","int main(){\n","\n","  int *h_array;\n","  alloc_bytes(h_array, ds*sizeof(h_array[0]));\n","  cudaCheckErrors(\"cudaMallocManaged Error\");\n","  memset(h_array, 0, ds*sizeof(h_array[0]));\n","  cudaMemPrefetchAsync(h_array, ds*sizeof(h_array[0]), 0); // add in step 2c\n","  inc<<<256, 256>>>(h_array, ds);\n","  cudaCheckErrors(\"kernel launch error\");\n","  cudaMemPrefetchAsync(h_array, ds*sizeof(h_array[0]), cudaCpuDeviceId); // add in step 2c\n","  cudaDeviceSynchronize();\n","  cudaCheckErrors(\"kernel execution error\");\n","  for (int i = 0; i < ds; i++)\n","    if (h_array[i] != 1) {printf(\"mismatch at %d, was: %d, expected: %d\\n\", i, h_array[i], 1); return -1;}\n","  printf(\"success!\\n\");\n","  return 0;\n","}"],"metadata":{"id":"QnYNa9nNFWLx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741344342644,"user_tz":-330,"elapsed":557,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"8e2ae738-2fe5-460f-9dd0-1fdcc1fbbd9a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing array_inc.cu\n"]}]},{"cell_type":"code","source":["! nvcc -arch=sm_75 array_inc.cu -o array_inc\n","! ./array_inc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDQhcJQsMfVi","executionInfo":{"status":"ok","timestamp":1741344352394,"user_tz":-330,"elapsed":4255,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"7c1e6e6b-e521-4669-af01-06c46a9fbe6f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["success!\n"]}]},{"cell_type":"markdown","source":["Why Use cudaMemPrefetchAsync()?\\\n","\n","Unified Memory (UM) can migrate pages between CPU and GPU.\n","If h_array is initially on CPU, accessing it from GPU triggers costly page migrations.\n","cudaMemPrefetchAsync(h_array, size, 0) moves h_array to GPU before kernel execution, reducing overhead.\n","After the kernel, cudaMemPrefetchAsync(h_array, size, cudaCpuDeviceId); brings data back to CPU for checking.\n","\n","📌 When Should You Use cudaMemPrefetchAsync()?\\\n","✅ If using Unified Memory (cudaMallocManaged) for better memory placement.\\\n","✅ Before a kernel launch: Move data to GPU first for better performance.\\\n","✅ After GPU computation: Move data back to CPU for processing."],"metadata":{"id":"22oE8OA5K3wg"}}]}