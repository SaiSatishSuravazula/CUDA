{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#1.1 Printing Threads and Blocks\n","These exercises will have you write some basic CUDA applications. You will learn how to allocate GPU memory, move data between the host and the GPU, and launch kernels.\n","\n","1. Hello World\n","Your first task is to create a simple hello world application in CUDA. The code skeleton is already given to you in hello.cu. Edit that file, paying attention to the FIXME locations, so that the output when run is like this:\n","\n","`Hello from block: 0, thread: 0`\n","\n","`Hello from block: 0, thread: 1`\n","\n","`Hello from block: 1, thread: 0`\n","\n","`Hello from block: 1, thread: 1`\n","\n","(the ordering of the above lines may vary; ordering differences do not indicate an incorrect result)\n","\n","Note the use of cudaDeviceSynchronize() after the kernel launch. In CUDA, kernel launches are asynchronous to the host thread. The host thread will launch a kernel but not wait for it to finish, before proceeding with the next line of host code. Therefore, to prevent application termination before the kernel gets to print out its message, we must use this synchronization function.\n","\n","After editing the code, compile it using the following:\n","\n","module load cuda\n","nvcc -o hello hello.cu\n","The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. nvcc is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n","\n","If you have trouble, you can look at hello_solution.cu for a complete example.\n","\n","To run your code at OLCF on Summit, we will use an LSF command:\n","\n","bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./hello\n","Alternatively, you may want to create an alias for your bsub command in order to make subsequent runs easier:\n","\n","alias lsfrun='bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1'\n","lsfrun ./hello\n","To run your code at NERSC on Cori, we can use Slurm:\n","\n","module load esslurm\n","srun -C gpu -N 1 -n 1 -t 10 -A m3502 --gres=gpu:1 -c 10 ./hello\n","Allocation m3502 is a custom allocation set up on Cori for this training series, and should be available to participants who registered in advance until January 18, 2020. If you cannot submit using this allocation, but already have access to another allocation that grants access to the Cori GPU nodes, you may use that instead.\n","\n","If you prefer, you can instead reserve a GPU in an interactive session, and then run an executable any number of times while the Slurm allocation is active:\n","\n","salloc -C gpu -N 1 -t 60 -A m3502 --gres=gpu:1 -c 10\n","srun -n 1 ./hello\n","Note that you only need to module load esslurm once per login session; this is what enables you to submit to the Cori GPU nodes.\n","\n"],"metadata":{"id":"v0OiQB3Cb7YD"}},{"cell_type":"markdown","source":["# hello.cu\n","\n"," include <stdio.h>\n","\n","__global__ void hello(){\n","\n","  printf(\"Hello from block: %u, thread: %u\\n\", FIXME);\n","}\n","\n","int main(){\n","\n","  hello<<<FIXME>>>();\n","  cudaDeviceSynchronize();\n","}\n"],"metadata":{"id":"mnOH-VatdyYk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bIjO0Ftb4WI","outputId":"a81e9de5-9fa8-4c0f-cf20-ce854ea8eee2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing hello.cu\n"]}],"source":["\n","\n","%%writefile hello.cu\n","\n","# include <stdio.h>\n","\n","__global__ void hello(){\n","\n","  printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n","}\n","\n","int main(){\n","  hello<<<2,2>>>();\n","  cudaDeviceSynchronize();\n","  return 0; //Add return statement to main function.\n","}\n"]},{"cell_type":"code","metadata":{"id":"nW4NEuImTzYu"},"source":["!nvcc  hello.cu -o hello\n","!./hello"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![cuda hw 1.1.png](data:image/png;base64,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)"],"metadata":{"id":"wVKpGyQ6rTcZ"}},{"cell_type":"markdown","source":["# 1.2 Vector Addition\n","\n","If you're up for a challenge, see if you can write a complete vector add program from scratch. Or if you prefer, there is a skeleton code given to you in `vector_add.cu`. Edit the code to build a complete vector_add program. Compile it and run it similar to the method given in exercise 1. You can refer to `vector_add_solution.cu` for a complete example.\n","\n","Note that this skeleton code includes something we didn't cover in lesson 1: CUDA error checking. Every CUDA runtime API call returns an error code. It's good practice (especially if you're having trouble) to rigorously check these error codes. A macro is given that will make this job easier. Note the special error checking method after a kernel call.\n","\n","Typical output when complete would look like this:\n","\n","`A[0] = 0.840188`\n","\n","`B[0] = 0.394383`\n","\n","`C[0] = 1.234571`"],"metadata":{"id":"sZDnLXUAr30a"}},{"cell_type":"markdown","source":["##vector_add.cu\n","\n","\n","\n","```\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 4096;\n","const int block_size = 256;  // CUDA maximum is 1024\n","// vector add kernel: C = A + B\n","__global__ void vadd(const float *A, const float *B, float *C, int ds){\n","\n","  int idx = FIXME // create typical 1D thread index from built-in variables\n","  if (idx < ds)\n","    FIXME         // do the vector (element) add here\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n","  h_B = new float[DSIZE];\n","  h_C = new float[DSIZE];\n","  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n","    h_A[i] = rand()/(float)RAND_MAX;\n","    h_B[i] = rand()/(float)RAND_MAX;\n","    h_C[i] = 0;}\n","  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n","  FIXME // allocate device space for vector B\n","  FIXME // allocate device space for vector C\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy vector A to device:\n","  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  // copy vector B to device:\n","  FIXME\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  vadd<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector C from device to host:\n","  FIXME\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  printf(\"A[0] = %f\\n\", h_A[0]);\n","  printf(\"B[0] = %f\\n\", h_B[0]);\n","  printf(\"C[0] = %f\\n\", h_C[0]);\n","  return 0;\n","}\n","  \n","```\n","\n","\n","  "],"metadata":{"id":"nOJzsIlkshqq"}},{"cell_type":"code","source":["%%writefile vector_add.cu\n","\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 4096;\n","const int block_size = 256;  // CUDA maximum is 1024\n","// vector add kernel: C = A + B\n","__global__ void vadd(const float *A, const float *B, float *C, int ds){\n","\n","  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n","  if (idx < ds)\n","    C[idx] = A[idx] + B[idx];\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","  h_A = new float[DSIZE];\n","  h_B = new float[DSIZE];\n","  h_C = new float[DSIZE];\n","  for (int i = 0; i < DSIZE; i++){\n","    h_A[i] = rand()/(float)RAND_MAX;\n","    h_B[i] = rand()/(float)RAND_MAX;\n","    h_C[i] = 0;}\n","  cudaMalloc(&d_A, DSIZE*sizeof(float));\n","  cudaMalloc(&d_B, DSIZE*sizeof(float));\n","  cudaMalloc(&d_C, DSIZE*sizeof(float));\n","  cudaCheckErrors(\"cudaMalloc failure\");\n","  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  vadd<<<(DSIZE+block_size-1)/block_size, block_size>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  printf(\"A[0] = %f\\n\", h_A[0]);\n","  printf(\"B[0] = %f\\n\", h_B[0]);\n","  printf(\"C[0] = %f\\n\", h_C[0]);\n","  return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibFykLPAlSW_","outputId":"573bd2b3-5d64-4e12-8e1f-44a0a334d31f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing vector_add.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 vector_add.cu -o vector_add\n","#need to specify -arch = sm_75 for tesla gpus and my nvcc is 11.8 - not compatible with 12 version.\n"],"metadata":{"id":"bOJcZSD4ugGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./vector_add"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpW78-XdvCK0","outputId":"4e3e225c-cfba-4be8-9dc0-c88731b78a75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A[0] = 0.840188\n","B[0] = 0.394383\n","C[0] = 1.234571\n"]}]},{"cell_type":"code","source":["!nvcc --ptxas-options=-v vector_add.cu -o vector_add\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qlkKIME0yZGz","outputId":"bd933fa0-f20c-4753-b08b-dac22d609cc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ptxas info    : 0 bytes gmem\n","ptxas info    : Compiling entry function '_Z4vaddPKfS0_Pfi' for 'sm_52'\n","ptxas info    : Function properties for _Z4vaddPKfS0_Pfi\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 8 registers, 348 bytes cmem[0]\n"]}]},{"cell_type":"markdown","source":["#1.3 Matrix Multiply (naive)\n","\n","A skeleton naive matrix multiply is given to you in matrix_mul.cu. See if you can complete it to get a correct result. If you need help, you can refer to matrix_mul_solution.cu.\n","\n","This example introduces 2D threadblock/grid indexing, something we did not cover in lesson 1. If you study the code you will probably be able to see how it is a structural extension from the 1D case.\n","\n","This code includes built-in error checking, so a correct result is indicated by the program."],"metadata":{"id":"ehEURITW4DLV"}},{"cell_type":"markdown","source":["##matrix_multiply.cu\n","\n","```\n","#include <stdio.h>\n","\n","// these are just for timing measurments\n","#include <time.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 4096;\n","const int block_size = 16;  // CUDA maximum is 1024 *total* threads in block\n","const float A_val = 1.0f;\n","const float B_val = 2.0f;\n","\n","// matrix multiply (naive) kernel: C = A * B\n","__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n","\n","  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n","  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n","\n","  if ((idx < ds) && (idy < ds)){\n","    float temp = 0;\n","    for (int i = 0; i < ds; i++)\n","      temp += A[FIXME*ds+i] * B[i*ds+FIXME];   // dot product of row and column\n","    C[idy*ds+idx] = temp;\n","  }\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","\n","  // these are just for timing\n","  clock_t t0, t1, t2;\n","  double t1sum=0.0;\n","  double t2sum=0.0;\n","\n","  // start timing\n","  t0 = clock();\n","\n","  h_A = new float[DSIZE*DSIZE];\n","  h_B = new float[DSIZE*DSIZE];\n","  h_C = new float[DSIZE*DSIZE];\n","  for (int i = 0; i < DSIZE*DSIZE; i++){\n","    h_A[i] = A_val;\n","    h_B[i] = B_val;\n","    h_C[i] = 0;}\n","\n","  // Initialization timing\n","  t1 = clock();\n","  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n","  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n","\n","  // Allocate device memory and copy input data over to GPU\n","  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n","  cudaCheckErrors(\"cudaMalloc failure\");\n","  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","\n","  // Cuda processing sequence step 1 is complete\n","\n","  // Launch kernel\n","  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n","  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n","  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","\n","  // Cuda processing sequence step 2 is complete\n","\n","  // Copy results back to host\n","  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","\n","  // GPU timing\n","  t2 = clock();\n","  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n","  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n","\n","  // Cuda processing sequence step 3 is complete\n","\n","  // Verify results\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n","  printf(\"Success!\\n\");\n","\n","  return 0;\n","}\n","  \n","\n","\n","```\n","\n","\n","\n","\n","\n","\n","  "],"metadata":{"id":"kH1P9SqT4c5w"}},{"cell_type":"code","source":["\n","%%writefile matrix_multiply.cu\n","# include <stdio.h>\n","\n","// these are just for timing measurments\n","# include <time.h>\n","\n","// error checking macro\n","# define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 8192;\n","const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n","const float A_val = 3.0f;\n","const float B_val = 2.0f;\n","\n","// matrix multiply (naive) kernel: C = A * B\n","__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n","\n","  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n","  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n","\n","  if ((idx < ds) && (idy < ds)){\n","    float temp = 0;\n","    for (int i = 0; i < ds; i++)\n","      temp += A[idy*ds+i] * B[i*ds+idx];   // dot product of row and column\n","    C[idy*ds+idx] = temp;\n","  }\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","\n","  // these are just for timing\n","  clock_t t0, t1, t2;\n","  double t1sum=0.0;\n","  double t2sum=0.0;\n","\n","  // start timing\n","  t0 = clock();\n","\n","  h_A = new float[DSIZE*DSIZE];\n","  h_B = new float[DSIZE*DSIZE];\n","  h_C = new float[DSIZE*DSIZE];\n","  for (int i = 0; i < DSIZE*DSIZE; i++){\n","    h_A[i] = A_val;\n","    h_B[i] = B_val;\n","    h_C[i] = 0;}\n","\n","  // Initialization timing\n","  t1 = clock();\n","  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n","  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n","\n","  // Allocate device memory and copy input data over to GPU\n","  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n","  cudaCheckErrors(\"cudaMalloc failure\");\n","  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","\n","  // Cuda processing sequence step 1 is complete\n","\n","  // Launch kernel\n","  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n","  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n","  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","\n","  // Cuda processing sequence step 2 is complete\n","\n","  // Copy results back to host\n","  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","\n","  // GPU timing\n","  t2 = clock();\n","  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n","  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n","\n","  // Cuda processing sequence step 3 is complete\n","\n","  // Verify results\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n","  printf(\"Success!\\n\");\n","\n","  return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alLWTrBG4BOQ","outputId":"d7b2b041-e1f3-41ed-8d33-2d85b1716f74","executionInfo":{"status":"ok","timestamp":1741236353678,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing matrix_multiply.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 matrix_multiply.cu -o matrix_multiply"],"metadata":{"id":"GdPo8ees_uzp","executionInfo":{"status":"ok","timestamp":1741236363037,"user_tz":-330,"elapsed":2970,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!./matrix_multiply"],"metadata":{"id":"lZlb6F1WACsT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"87d29157-db83-42a5-dd8b-e393029efc5e","executionInfo":{"status":"ok","timestamp":1741236369267,"user_tz":-330,"elapsed":3552,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Init took 0.691415 seconds.  Begin compute\n","Done. Compute took 2.102869 seconds\n","Success!\n"]}]}]}