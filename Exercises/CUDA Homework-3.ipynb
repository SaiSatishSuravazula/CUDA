{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN6+y4bzk4lQZkZu2rpwOfi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["###3.1 Vector Add\n","\n","####Introducing grid-stride loop\n","\n","We'll use a slight variation on the vector add code presented in a previous homework (vector_add.cu). Edit the code to build a complete vector_add program. You can refer to vector_add_solution.cu for a complete example. For this example, we have made a change to the kernel to use something called a grid-stride loop. This topic will be dealt with in more detail in a later training session, but for now we can describe it as a flexible kernel design method that allows a simple kernel to handle an arbitrary size data set with an arbitrary size \"grid\", i.e. the configuration of blocks and threads associated with the kernel launch. If you'd like to read more about grid-stride loops right now, you can visit https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/\n","\n","\n","As we will see, this flexibility is important for our investigations in section 2 of this homework session. However, as before, all you need to focus on are the FIXME items, and these sections will be identical to the work you did in a previous homework assignment. If you get stuck, you can refer to the solution vector_add_solution.cu.\n","\n","Note that this skeleton code includes something we didn't cover in lesson 1: CUDA error checking. Every CUDA runtime API call returns an error code. It's good practice (especially if you're having trouble) to rigorously check these error codes. A macro is given that will make this job easier. Note the special error checking method after a kernel call"],"metadata":{"id":"Q6q294G0zqZk"}},{"cell_type":"markdown","source":["##vector_add.cu\n","\n","```\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 32*1048576;\n","// vector add kernel: C = A + B\n","__global__ void vadd(const float *A, const float *B, float *C, int ds){\n","\n","  for (int idx = threadIdx.x+blockDim.x*blockIdx.x; idx < ds; idx+=gridDim.x*blockDim.x)         // a grid-stride loop\n","    FIXME         // do the vector (element) add here\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n","  h_B = new float[DSIZE];\n","  h_C = new float[DSIZE];\n","  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n","    h_A[i] = rand()/(float)RAND_MAX;\n","    h_B[i] = rand()/(float)RAND_MAX;\n","    h_C[i] = 0;}\n","  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n","  FIXME // allocate device space for vector B\n","  FIXME // allocate device space for vector C\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy vector A to device:\n","  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  // copy vector B to device:\n","  FIXME\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  int blocks = 1;  // modify this line for experimentation\n","  int threads = 1; // modify this line for experimentation\n","  vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector C from device to host:\n","  FIXME\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  printf(\"A[0] = %f\\n\", h_A[0]);\n","  printf(\"B[0] = %f\\n\", h_B[0]);\n","  printf(\"C[0] = %f\\n\", h_C[0]);\n","  return 0;\n","}\n","```"],"metadata":{"id":"Hc0XuRkzuqgZ"}},{"cell_type":"code","source":["%%writefile vector_add.cu\n","#include <stdio.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 32*1048576;\n","// vector add kernel: C = A + B\n","__global__ void vadd(const float *A, const float *B, float *C, int ds){\n","\n","  for (int idx = threadIdx.x+blockDim.x*blockIdx.x; idx < ds; idx+=gridDim.x*blockDim.x)         // a grid-stride loop\n","  {// do the vector (element) add here\n","    C[idx] = A[idx] + B[idx];\n","  }\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","  h_A = new float[DSIZE];  // allocate space for vectors in host memory\n","  h_B = new float[DSIZE];\n","  h_C = new float[DSIZE];\n","  for (int i = 0; i < DSIZE; i++){  // initialize vectors in host memory\n","    h_A[i] = rand()/(float)RAND_MAX;\n","    h_B[i] = rand()/(float)RAND_MAX;\n","    h_C[i] = 0;}\n","  cudaMalloc(&d_A, DSIZE*sizeof(float));  // allocate device space for vector A\n","  cudaMalloc(&d_B, DSIZE*sizeof(float));  // allocate device space for vector B\n","  cudaMalloc(&d_C, DSIZE*sizeof(float));  // allocate device space for vector C\n","  cudaCheckErrors(\"cudaMalloc failure\"); // error checking\n","  // copy vector A to device:\n","  cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  // copy vector B to device:\n","  cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","  //cuda processing sequence step 1 is complete\n","  int blocks = 1;  // modify this line for experimentation\n","  int threads = 1; // modify this line for experimentation\n","  vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","  //cuda processing sequence step 2 is complete\n","  // copy vector C from device to host:\n","  cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","  //cuda processing sequence step 3 is complete\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  printf(\"A[0] = %f\\n\", h_A[0]);\n","  printf(\"B[0] = %f\\n\", h_B[0]);\n","  printf(\"C[0] = %f\\n\", h_C[0]);\n","  return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F374ETHNup6A","executionInfo":{"status":"ok","timestamp":1741254103124,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"8008fe1b-16b5-4d26-b333-ff439c646ce0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting vector_add.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 vector_add.cu -o vector_add"],"metadata":{"id":"UVqS0RclzqRV","executionInfo":{"status":"ok","timestamp":1741254107648,"user_tz":-330,"elapsed":1581,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!./vector_add"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kgdd7f5bz3Ih","executionInfo":{"status":"ok","timestamp":1741254115182,"user_tz":-330,"elapsed":4685,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"5bb0ff7f-b57e-412b-b0e8-bde724489671"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["A[0] = 0.840188\n","B[0] = 0.394383\n","C[0] = 1.234571\n"]}]},{"cell_type":"markdown","source":["Notice that the stride of the loop is blockDim.x * gridDim.x which is the total number of threads in the grid. So if there are 1280 threads in the grid, thread 0 will compute elements 0, 1280, 2560, etc. This is why I call this a grid-stride loop. By using a loop with stride equal to the grid size, we ensure that all addressing within warps is unit-stride, so we get maximum memory coalescing, just as in the monolithic version.\n","\n","When launched with a grid large enough to cover all iterations of the loop, the grid-stride loop should have essentially the same instruction cost as the if statement in the monolithic kernel, because the loop increment will only be evaluated when the loop condition evaluates to true.\n","\n","There are several benefits to using a grid-stride loop.\n","\n","**Scalability and thread reuse.**\n","\n","By using a loop, you can support any problem size even if it exceeds the largest grid size your CUDA device supports. Moreover, you can limit the number of blocks you use to tune performance. For example, itâ€™s often useful to launch a number of blocks that is a multiple of the number of multiprocessors on the device, to balance utilization. As an example, we might launch the loop version of the kernel like this.\n","```\n","int numSMs;\n","cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, devId);\n","// Perform SAXPY on 1M elements\n","saxpy<<<32*numSMs, 256>>>(1 << 20, 2.0, x, y);\n","```\n","\n","When you limit the number of blocks in your grid, threads are reused for multiple computations. Thread reuse amortizes thread creation and destruction cost along with any other processing the kernel might do before or after the loop (such as thread-private or shared data initialization).\n","\n","**Debugging.**\n","\n"," By using a loop instead of a monolithic kernel, you can easily switch to serial processing by launching one block with one thread.\n","\n","```saxpy<<<1,1>>>(1<<20, 2.0, x, y);```\n","\n","This makes it easier to emulate a serial host implementation to validate results, and it can make printf debugging easier by serializing the print order. Serializing the computation also allows you to eliminate numerical variations caused by changes in the order of operations from run to run, helping you to verify that your numerics are correct before tuning the parallel version.\n","\n","**Portability and readability.**\n","\n","The grid-stride loop code is more like the original sequential loop code than the monolithic kernel code, making it clearer for other users. In fact we can pretty easily write a version of the kernel that compiles and runs either as a parallel CUDA kernel on the GPU or as a sequential loop on the CPU. The Hemi library provides a `grid_stride_range()` helper that makes this trivial using C++11 range-based for loops.\n","HEMI_LAUNCHABLE\n","\n","```\n","void saxpy(int n, float a, float *x, float *y)\n","{\n","  for (auto i : hemi::grid_stride_range(0, n)) {\n","    y[i] = a * x[i] + y[i];\n","  }\n","}\n","```\n","\n","We can launch the kernel using this code, which generates a kernel launch when compiled for CUDA, or a function call when compiled for the CPU.\n","```\n","hemi::cudaLaunch(saxpy, 1<<20, 2.0, x, y);\n","```\n","Grid-stride loops are a great way to make your CUDA kernels flexible, scalable, debuggable, and even portable. While the examples in this post have all used CUDA C/C++, the same concepts apply in other CUDA languages such as CUDA Fortran."],"metadata":{"id":"DWZZ_c0S1oqV"}},{"cell_type":"markdown","source":["###3.2 Profiling Experiments\n","\n","Our objective now will be to explore some of the concepts we learned in the lesson. In particular we want to see what effect grid sizing (choice of blocks, and threads per block) have on performance. We could do analysis like this using host-code-based timing methods, but we'll introduce a new concept, using a GPU profiler. In a future session, you'll learn more about the GPU profilers (Nsight Compute and Nsight Systems), but for now we will use Nsight Compute in a fairly simple fashion to get some basic data about kernel behavior, to use for comparison. (If you'd like to read more about the Nsight profilers, you can start here: https://devblogs.nvidia.com/migrating-nvidia-nsight-tools-nvvp-nvprof/)\n","\n","\n","First, note that the code has these two lines in it:\n","```\n","  int blocks = 1;  // modify this line for experimentation\n","  int threads = 1; // modify this line for experimentation\n","```\n","These lines control the grid sizing. The first variable blocks chooses the total number of blocks to launch. The second variable threads chooses the number of threads per block to launch. This second variable must be constrained to choices between 1 and 1024, inclusive. These are limits imposed by the GPU hardware.\n","\n","Let's consider 3 cases. In each case, we will modify the blocks and threads variables, recompile the code, and then run the code under the Nsight Compute profiler.\n","\n","Nsight Compute is installed as part of newer CUDA toolkits (10.1 and newer), but the path to the command line tool may or may not be set up as part of your CUDA install. Therefore it may be necessary to specify the complete command line to access the tool. We will demonstrate that here with our invocations.\n","\n","For the following profiler experiments, we will assume you have loaded the profile module and acquired a node for interactive usage:\n","\n","\n","\n"],"metadata":{"id":"qAHUfl3H25Cz"}},{"cell_type":"code","source":["!nvprof ./vector_add"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3ZmTjl53b9v","executionInfo":{"status":"ok","timestamp":1741257090758,"user_tz":-330,"elapsed":218,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"95b676a0-fe88-47ae-86d8-0a8795811859"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["======== Error: application not found.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s4SWaEzFATv3"},"execution_count":null,"outputs":[]}]}