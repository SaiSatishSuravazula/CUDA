{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhraR44yU1eguKbWyrlIRe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#9.1. Exploring Threadblock-Level Groups\n","\n","##1a. Creating Groups\n","\n","First, you should take the task1.cu code, and complete the sections indicated by FIXME to provide a proper thread-block group, and assign that group to the group being used for printout purposes. You should only need to modify the 2 lines containing FIXME for this first step.\n","\n","You can compile your code as follows:\n","\n","nvcc -arch=sm_70 -o task1 task1.cu -std=c++11\n","\n","nvcc is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++. Note that because we're using C++11 (which is required for cooperative groups) we need a sufficiently modern compiler (gcc >= 5 should be sufficient).\n","\n","Correct output should look like this:\n","\n","```\n","group partial sum: 256\n","```\n","If you need help, refer to the task1_solution1.cu file. (which contains the solution for tasks 1a, 1b, and 1c)\n","\n","##1b. Partitioning Groups\n","\n","Next uncomment the next line that starts with the auto keyword, and complete that line to use the previously created thread block group and subdivide it into a set of 32-thread partitions, using the dynamic (runtime) partitioning method.\n","\n","Compile and run the code as above. correct output should look like:\n","\n","```\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","```\n","##1c. Third Group Creation/Decomposition\n","\n","Now perform the 3rd group creation/decomposition.\n","\n","Compile and run the code as above. Correct output should look like:\n","\n","```\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","```"],"metadata":{"id":"JvumAX5rMHLp"}},{"cell_type":"markdown","source":["##task_1.cu\n","\n","```\n","#include <cooperative_groups.h>\n","#include <stdio.h>\n","using namespace cooperative_groups;\n","const int nTPB = 256;\n","__device__ int reduce(thread_group g, int *x, int val) {\n","  int lane = g.thread_rank();\n","  for (int i = g.size()/2; i > 0; i /= 2) {\n","    x[lane] = val;       g.sync();\n","    if (lane < i) val += x[lane + i];  g.sync();\n","  }\n","  if (g.thread_rank() == 0) printf(\"group partial sum: %d\\n\", val);\n","  return val;\n","}\n","\n","__global__ void my_reduce_kernel(int *data){\n","\n","  __shared__ int sdata[nTPB];\n","  // task 1a: create a proper thread block group below\n","  auto g1 = FIXME\n","  size_t gindex = g1.group_index().x * nTPB + g1.thread_index().x;\n","  // task 1b: uncomment and create a proper 32-thread tile below, using group g1 created above\n","  // auto g2 = FIXME\n","  // task 1c: uncomment and create a proper 16-thread tile below, using group g2 created above\n","  // auto g3 = FIXME\n","  // for each task, adjust the group to point to the last group created above\n","  auto g = FIXME\n","  // Make sure we send in the appropriate patch of shared memory\n","  int sdata_offset = (g1.thread_index().x / g.size()) * g.size();\n","  reduce(g, sdata + sdata_offset, data[gindex]);\n","}\n","\n","int main(){\n","\n","  int *data;\n","  cudaMallocManaged(&data, nTPB*sizeof(data[0]));\n","  for (int i = 0; i < nTPB; i++) data[i] = 1;\n","  my_reduce_kernel<<<1,nTPB>>>(data);\n","  cudaError_t err = cudaDeviceSynchronize();\n","  if (err != cudaSuccess) printf(\"cuda error: %s\\n\", cudaGetErrorString(err));\n","}\n","\n","```"],"metadata":{"id":"7DEUnQNpNZ3F"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM7ylI8OIghV","executionInfo":{"status":"ok","timestamp":1741621871429,"user_tz":-330,"elapsed":908,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"221139b9-3533-4fb8-eb91-bf1515b2e32c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing task_1c.cu\n"]}],"source":["%%writefile task_1c.cu\n","# include <cooperative_groups.h>\n","# include <stdio.h>\n","using namespace cooperative_groups;\n","const int nTPB = 256;\n","__device__ int reduce(thread_group g, int *x, int val) {\n","  int lane = g.thread_rank();\n","  for (int i = g.size()/2; i > 0; i /= 2) {\n","    x[lane] = val;       g.sync();\n","    if (lane < i) val += x[lane + i];  g.sync();\n","  }\n","  if (g.thread_rank() == 0) printf(\"group partial sum: %d\\n\", val);\n","  return val;\n","}\n","\n","__global__ void my_reduce_kernel(int *data){\n","\n","  __shared__ int sdata[nTPB];\n","  // task 1a: create a proper thread block group below\n","  auto g1 = this_thread_block();\n","  size_t gindex = g1.group_index().x * nTPB + g1.thread_index().x;\n","  // task 1b: uncomment and create a proper 32-thread tile below, using group g1 created above\n","  auto g2 = tiled_partition(g1,32);\n","  // task 1c: uncomment and create a proper 16-thread tile below, using group g2 created above\n","  auto g3 = tiled_partition(g2,16);\n","  // for each task, adjust the group to point to the last group created above\n","  auto g = g3;\n","  // Make sure we send in the appropriate patch of shared memory\n","  int sdata_offset = (g1.thread_index().x / g.size()) * g.size();\n","  reduce(g, sdata + sdata_offset, data[gindex]);\n","}\n","\n","int main(){\n","\n","  int *data;\n","  cudaMallocManaged(&data, nTPB*sizeof(data[0]));\n","  for (int i = 0; i < nTPB; i++) data[i] = 1;\n","  my_reduce_kernel<<<1,nTPB>>>(data);\n","  cudaError_t err = cudaDeviceSynchronize();\n","  if (err != cudaSuccess) printf(\"cuda error: %s\\n\", cudaGetErrorString(err));\n","}\n"]},{"cell_type":"code","source":["!nvcc -arch=sm_70 -o task_1 task_1.cu -std=c++11"],"metadata":{"id":"xXC0uZOaHIyY","executionInfo":{"status":"ok","timestamp":1741510671938,"user_tz":-330,"elapsed":4489,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!./task_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H96OSzf0IAlJ","executionInfo":{"status":"ok","timestamp":1741510687795,"user_tz":-330,"elapsed":974,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"3259f427-0b05-4c34-ceca-7cf8fe043a69"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["group partial sum: 256\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_70 -o task_1b task_1b.cu -std=c++11"],"metadata":{"id":"Ae_ZPDQineSL","executionInfo":{"status":"ok","timestamp":1741621822813,"user_tz":-330,"elapsed":5971,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!./task_1b\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIdy6dj_wBkB","executionInfo":{"status":"ok","timestamp":1741621842971,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"fa131bc0-8fdf-4ac5-8fe8-8ad9aa59a911"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n","group partial sum: 32\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_70 -o task_1c task_1c.cu"],"metadata":{"id":"Xden5vscwEk3","executionInfo":{"status":"ok","timestamp":1741621899114,"user_tz":-330,"elapsed":4995,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!./task_1c"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cI9ZN5cwTmq","executionInfo":{"status":"ok","timestamp":1741621922196,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"0949470b-41ab-4162-8262-de04961e4bd3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n","group partial sum: 16\n"]}]},{"cell_type":"markdown","source":["#9.2 Exploring Grid-Wide Sync\n","\n","One of the motivations suggested for a grid-wide sync is to combine algorithm phases which need to be completed in sequence, and would normally be realized with separate CUDA kernel calls. In this case, the kernel launch boundary provides an implicit/effective grid-wide sync. However cooperative groups provides the possibility of a grid wide sync directly in kernel code, rather than at a kernel launch boundary.\n","\n","One such algorithm would be stream compaction. Stream compaction is used in many places, and fundamentally seeks to reduce the length of a data stream using a particular removal heuristic or predicate test. For example, if we had the following data stream:\n","\n","```\n","3 4 3 7 0 5 0 8 0 0 0 4\n","```\n","\n","we could do stream compaction by removing the zeroes, ending up with:\n","\n","```\n","3 4 3 7 5 8 4\n","```\n","Like many reduction type algorithms (the output here is potentially much smaller than the input), we can easily imagine how to do this in a serial fashion, but a fast parallel stream compaction requires some additional thought. A common approach is to use a prefix sum. A prefix sum is a data set, where each data item in the set represents the sum of the previous input elements from the beginning of the input to that point. We can use a prefix sum to help parallelize our stream compaction. We start by creating an array of ones and zeroes, where there is a one corresponding to the element we want to keep, and zero for the element we want to discard:\n","\n","```\n","3 4 3 7 0 5 0 8 0 0 0 4 (input data)\n","1 1 1 1 0 1 0 1 0 0 0 1 (filtering of input)\n","```\n","\n","We then do an exclusive prefix sum on that filtered array (exclusive means only the elements \"to the left\" are included in the sum. The element at that position is excluded).\n","\n","```\n","3 4 3 7 0 5 0 8 0 0 0 4 (input data)\n","1 1 1 1 0 1 0 1 0 0 0 1 (filtering of input)\n","0 1 2 3 4 4 5 5 6 6 6 6 (exclusive prefix sum of filtered data)\n","```\n","\n","This prefix sum now contains the index into the output array that the input position should be copied to. We only copy a position from input to output if the corresponding filter element is not zero. This demonstrates how to use a prefix sum to assist with a stream compaction, but doesn't identify how to do the prefix sum in parallel, efficiently. A full treatment here is beyond the scope of this document, but you can refer here for a good treatise: https://people.eecs.berkeley.edu/~driscoll/cs267/papers/gpugems3_ch39.html Some key takeaways are that a prefix sum has a sweeping operation, not unlike the sweeping operation that is successively performed in a parallel reduction, but there are key differences. Two of these key differences are that the sweep is from \"left\" to \"right\" in the prefix sum whereas it is usually from right to left in a typical parallel reduction, and also that the break points (i.e. the division of threads participating at each sweep phase) is different.\n","\n","When parallelizing a prefix sum, we often require multiple phases, for example a thread-block level scan (prefix-sum) operation, followed by another operation to \"fix up\" the threadblock level results based on the data from other (\"previous\") thread blocks. These phases may require a grid-wide sync, and typical scan from a library such as thrust will use multiple kernel calls. Let's see if we can do it in a single kernel call. You won't have to write any scan code, other than inserting appropriate cooperative group sync points. We need sync points at the threadblock leve (based on the threadblock level group created for you) and also at the grid level.\n","\n","Start with the task2.cu code, and perform 2 things:\n","\n","Modify the FIXME statements in the kernel to insert appropriate sync operations as requested, based on the two group types created at the top of the kernel. Only one grid-wide sync point is needed, the others are all thread-block-level sync points.\n","In the host code, modify the FIXME statements to do a proper cooperative launch. The launch function is already provided, you just need to fill in the remaining 4 arguments. Refer to the task2_solution.cu file for help, or refer to the cuda runtime API documentation for the launch function: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g504b94170f83285c71031be6d5d15f73\n","Once you have made the above modification, compile your code as follows:"],"metadata":{"id":"sBzp3hmewTUC"}},{"cell_type":"code","source":[],"metadata":{"id":"ozQP73tHpFxr"},"execution_count":null,"outputs":[]}]}