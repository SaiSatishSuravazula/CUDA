{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMT77XiR1d5W3Bmx2M3gfOz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["These exercises will help reinforce the concept of Shared Memory on the GPU.\n","\n","#2.1 1D Stencil Using Shared Memory\n","\n","Your first task is to create a 1D stencil application that uses shared memory. The code skeleton is provided in stencil_1d.cu. Edit that file, paying attention to the FIXME locations. The code will verify output and report any errors.\n","\n","After editing the code, compile it using the following:\n","\n","```\n","module load cuda\n","nvcc -o stencil_1d stencil_1d.cu\n","```\n","\n","The module load command selects a CUDA compiler for your use. The module load command only needs to be done once per session/login. nvcc is the CUDA compiler invocation command. The syntax is generally similar to gcc/g++.\n","\n","To run your code, we will use an LSF command:\n","\n","```\n","bsub -W 10 -nnodes 1 -P <allocation_ID> -Is jsrun -n1 -a1 -c1 -g1 ./stencil_1d\n","```\n","\n"],"metadata":{"id":"Ii0Q8e9wwO_M"}},{"cell_type":"markdown","source":["###stencil_1d.cu\n","\n","```\n","#include <stdio.h>\n","#include <algorithm>\n","\n","using namespace std;\n","\n","#define N 4096\n","#define RADIUS 3\n","#define BLOCK_SIZE 16\n","\n","__global__ void stencil_1d(int *in, int *out) {\n","    __shared__ int temp[FIXME];\n","    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int lindex = FIXME;\n","\n","    // Read input elements into shared memory\n","    temp[lindex] = in[gindex];\n","    if (threadIdx.x < RADIUS) {\n","      temp[lindex - RADIUS] = in[gindex - RADIUS];\n","      temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n","    }\n","\n","    // Synchronize (ensure all the data is available)\n","    __syncthreads();\n","\n","    // Apply the stencil\n","    int result = 0;\n","    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n","      result += temp[FIXME];\n","\n","    // Store the result\n","    out[gindex] = result;\n","}\n","\n","void fill_ints(int *x, int n) {\n","  fill_n(x, n, 1);\n","}\n","\n","int main(void) {\n","  int *in, *out; // host copies of a, b, c\n","  int *d_in, *d_out; // device copies of a, b, c\n","\n","  // Alloc space for host copies and setup values\n","  int size = (FIXME) * sizeof(int);\n","  in = (int *)malloc(size); fill_ints(in, N + 2*RADIUS);\n","  out = (int *)malloc(size); fill_ints(out, N + 2*RADIUS);\n","\n","  // Alloc space for device copies\n","  cudaMalloc((void **)&d_in, size);\n","  cudaMalloc((void **)&d_out, size);\n","\n","  // Copy to device\n","  cudaMemcpy(d_in, in, size, cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_out, out, size, cudaMemcpyHostToDevice);\n","\n","  // Launch stencil_1d() kernel on GPU\n","  stencil_1d<<<N/BLOCK_SIZE,BLOCK_SIZE>>>(FIXME, FIXME);\n","\n","  // Copy result back to host\n","  cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);\n","\n","  // Error Checking\n","  for (int i = 0; i < N + 2*RADIUS; i++) {\n","    if (i<RADIUS || i>=N+RADIUS){\n","      if (out[i] != 1)\n","    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1);\n","    } else {\n","      if (out[i] != 1 + 2*RADIUS)\n","    \tprintf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1 + 2*RADIUS);\n","    }\n","  }\n","\n","  // Cleanup\n","  free(in); free(out);\n","  cudaFree(d_in); cudaFree(d_out);\n","  printf(\"Success!\\n\");\n","  return 0;\n","}\n","```"],"metadata":{"id":"TRB1d3zqxEgI"}},{"cell_type":"markdown","source":["We have input and ouputs of size of size N+2*RADIUS where N is taken as 4096 for this problem.\n","\n","The fill_n function is part of the C++ Standard Library (<algorithm>).\n","```\n","fill_n(start_iterator, count, value);\n","```\n","What It Does:\n","\n","ðŸ”¹ Fills the first count elements starting from start_iterator with value.\n","\n","\n","We first fill the input and output elements with value 1 for this problem."],"metadata":{"id":"Hb3AtJ1v0ufs"}},{"cell_type":"code","source":["%%writefile stencil_1d.cu\n","# include <stdio.h>\n","# include <algorithm>\n","\n","using namespace std;\n","\n","# define N 4096\n","# define RADIUS 3\n","# define BLOCK_SIZE 16\n","\n","__global__ void stencil_1d(int *in, int *out) {\n","    //receives the pointer of d_in and d_out after radius elements\n","\n","    __shared__ int temp[BLOCK_SIZE+ 2*RADIUS];\n","    int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int lindex = threadIdx.x + RADIUS;\n","\n","    /*\n","      This means in[0] refers to d_in[RADIUS].\n","      in[gindex - RADIUS] refers to d_in[RADIUS - RADIUS] = d_in[0], which is valid.\n","      The real memory location accessed by in[gindex] is actually d_in[gindex + RADIUS].\n","\n","    */\n","\n","    // Read input elements into shared memory\n","    temp[lindex] = in[gindex];\n","    if (threadIdx.x < RADIUS) {\n","      temp[lindex - RADIUS] = in[gindex - RADIUS];\n","      temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n","    }\n","\n","    // Synchronize (ensure all the data is available)\n","    __syncthreads();\n","\n","    // Apply the stencil\n","    int result = 0;\n","    for (int offset = -RADIUS; offset <= RADIUS; offset++)\n","      result += temp[lindex+offset];\n","\n","    // Store the result\n","    out[gindex] = result;\n","}\n","\n","void fill_ints(int *x, int n) {\n","  fill_n(x, n, 1);\n","}\n","\n","int main(void) {\n","  int *in, *out; // host copies of a, b, c\n","  int *d_in, *d_out; // device copies of a, b, c\n","\n","  // Alloc space for host copies and setup values\n","  int size = (N + 2*RADIUS) * sizeof(int);\n","  in = (int *)malloc(size); fill_ints(in, N + 2*RADIUS);\n","  out = (int *)malloc(size); fill_ints(out, N + 2*RADIUS);\n","\n","  // Alloc space for device copies\n","  cudaMalloc((void **)&d_in, size);\n","  cudaMalloc((void **)&d_out, size);\n","\n","  // Copy to device\n","  cudaMemcpy(d_in, in, size, cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_out, out, size, cudaMemcpyHostToDevice);\n","\n","  // Launch stencil_1d() kernel on GPU\n","  // sending the pointer of d_in and d_out after RADIUS elements to the kernel.\n","  stencil_1d<<<N/BLOCK_SIZE,BLOCK_SIZE>>>(d_in + RADIUS, d_out + RADIUS);\n","\n","  // Copy result back to host\n","  cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);\n","\n","  // Error Checking\n","  for (int i = 0; i < N + 2*RADIUS; i++) {\n","    if (i<RADIUS || i>=N+RADIUS){\n","      if (out[i] != 1)\n","        printf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1);\n","    } else {\n","      if (out[i] != 1 + 2*RADIUS)\n","        printf(\"Mismatch at index %d, was: %d, should be: %d\\n\", i, out[i], 1 + 2*RADIUS);\n","    }\n","  }\n","\n","  // Cleanup\n","  free(in); free(out);\n","  cudaFree(d_in); cudaFree(d_out);\n","  printf(\"Success!\\n\");\n","  return 0;\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PygBfC0JyBfT","executionInfo":{"status":"ok","timestamp":1741191044100,"user_tz":-330,"elapsed":365,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"208882a7-e19d-48ec-e4a3-10add8f9fda0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting stencil_1d.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -o stencil_1d stencil_1d.cu"],"metadata":{"id":"lCbmEU4wwajQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./stencil_1d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsCvT-nQEygn","executionInfo":{"status":"ok","timestamp":1741191077020,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"d4b5a329-9c14-421c-de31-8eb7f70296c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Success!\n"]}]},{"cell_type":"markdown","source":["# 2.2 2D Matrix Multiply Using Shared Memory\n","\n","Next, let's apply shared memory to the 2D matrix multiply we wrote in Homework 1. FIXME locations are provided in the code skeleton in matrix_mul_shared.cu. See if you can successfully load the required data into shared memory and then appropriately update the dot product calculation. Compile and run your code using the following:\n","\n","```\n","module load cuda\n","nvcc -o matrix_mul matrix_mul_shared.cu\n","```\n","\n","Note that timing information is included. Go back and run your solution from Homework 1 and observe the runtime. What runtime impact do you notice after applying shared memory to this 2D matrix multiply? How does it differ from the runtime you observed in your previous implementation?"],"metadata":{"id":"KAGcmGvjnCTY"}},{"cell_type":"markdown","source":["##matrix_mul_shared.cu\n","\n","```\n","#include <stdio.h>\n","\n","// these are just for timing measurments\n","#include <time.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 8192;\n","const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n","const float A_val = 3.0f;\n","const float B_val = 2.0f;\n","\n","// matrix multiply (naive) kernel: C = A * B\n","__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n","\n","  // declare cache in shared memory\n","  __shared__ float As[block_size][block_size];\n","  __shared__ float Bs[block_size][block_size];\n","\n","  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n","  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n","\n","  if ((idx < ds) && (idy < ds)){\n","    float temp = 0;\n","    for (int i = 0; i < ds/block_size; i++) {\n","\n","      // Load data into shared memory\n","      As[threadIdx.y][threadIdx.x] = A[FIXME];\n","      Bs[threadIdx.y][threadIdx.x] = B[FIXME];\n","\n","      // Synchronize\n","      __syncthreads();\n","\n","      // Keep track of the running sum\n","      for (int k = 0; k < block_size; k++)\n","      \ttemp += As[FIXME][FIXME] * Bs[FIXME][FIXME]; // dot product of row and column\n","      __syncthreads();\n","\n","    }\n","\n","    // Write to global memory\n","    C[idy*ds+idx] = temp;\n","  }\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","\n","\n","  // these are just for timing\n","  clock_t t0, t1, t2;\n","  double t1sum=0.0;\n","  double t2sum=0.0;\n","\n","  // start timing\n","  t0 = clock();\n","\n","  h_A = new float[DSIZE*DSIZE];\n","  h_B = new float[DSIZE*DSIZE];\n","  h_C = new float[DSIZE*DSIZE];\n","  for (int i = 0; i < DSIZE*DSIZE; i++){\n","    h_A[i] = A_val;\n","    h_B[i] = B_val;\n","    h_C[i] = 0;}\n","\n","  // Initialization timing\n","  t1 = clock();\n","  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n","  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n","\n","  // Allocate device memory and copy input data over to GPU\n","  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n","  cudaCheckErrors(\"cudaMalloc failure\");\n","  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","\n","  // Cuda processing sequence step 1 is complete\n","\n","  // Launch kernel\n","  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n","  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n","  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","\n","  // Cuda processing sequence step 2 is complete\n","\n","  // Copy results back to host\n","  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","\n","  // GPU timing\n","  t2 = clock();\n","  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n","  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n","\n","  // Cuda processing sequence step 3 is complete\n","\n","  // Verify results\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n","  printf(\"Success!\\n\");\n","  return 0;\n","}\n","  \n","```"],"metadata":{"id":"xvctQTi3ncJh"}},{"cell_type":"code","source":["\n","%%writefile matrix_mul_shared.cu\n","#include <stdio.h>\n","\n","// these are just for timing measurments\n","#include <time.h>\n","\n","// error checking macro\n","#define cudaCheckErrors(msg) \\\n","    do { \\\n","        cudaError_t __err = cudaGetLastError(); \\\n","        if (__err != cudaSuccess) { \\\n","            fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n","                msg, cudaGetErrorString(__err), \\\n","                __FILE__, __LINE__); \\\n","            fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n","            exit(1); \\\n","        } \\\n","    } while (0)\n","\n","\n","const int DSIZE = 8192;\n","const int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\n","const float A_val = 3.0f;\n","const float B_val = 2.0f;\n","\n","// matrix multiply (naive) kernel: C = A * B\n","__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n","\n","  // declare cache in shared memory\n","  __shared__ float As[block_size][block_size];\n","  __shared__ float Bs[block_size][block_size];\n","\n","  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n","  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n","\n","  if ((idx < ds) && (idy < ds)){\n","    float temp = 0;\n","    for (int i = 0; i < ds/block_size; i++) {\n","\n","      // Load data into shared memory\n","      As[threadIdx.y][threadIdx.x] = A[(idy)*ds+(i*block_size)+threadIdx.x];\n","      Bs[threadIdx.y][threadIdx.x] = B[i*block_size*ds+threadIdx.y*ds+idx];\n","\n","      // Synchronize\n","      __syncthreads();\n","\n","      // Keep track of the running sum\n","      for (int k = 0; k < block_size; k++)\n","      \ttemp += As[threadIdx.y][k] * Bs[k][threadIdx.x]; // dot product of row and column\n","      __syncthreads();\n","\n","    }\n","\n","    // Write to global memory\n","    C[idy*ds+idx] = temp;\n","  }\n","}\n","\n","int main(){\n","\n","  float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n","\n","\n","  // these are just for timing\n","  clock_t t0, t1, t2;\n","  double t1sum=0.0;\n","  double t2sum=0.0;\n","\n","  // start timing\n","  t0 = clock();\n","\n","  h_A = new float[DSIZE*DSIZE];\n","  h_B = new float[DSIZE*DSIZE];\n","  h_C = new float[DSIZE*DSIZE];\n","  for (int i = 0; i < DSIZE*DSIZE; i++){\n","    h_A[i] = A_val;\n","    h_B[i] = B_val;\n","    h_C[i] = 0;}\n","\n","  // Initialization timing\n","  t1 = clock();\n","  t1sum = ((double)(t1-t0))/CLOCKS_PER_SEC;\n","  printf(\"Init took %f seconds.  Begin compute\\n\", t1sum);\n","\n","  // Allocate device memory and copy input data over to GPU\n","  cudaMalloc(&d_A, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_B, DSIZE*DSIZE*sizeof(float));\n","  cudaMalloc(&d_C, DSIZE*DSIZE*sizeof(float));\n","  cudaCheckErrors(\"cudaMalloc failure\");\n","  cudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaMemcpy(d_B, h_B, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\n","  cudaCheckErrors(\"cudaMemcpy H2D failure\");\n","\n","  // Cuda processing sequence step 1 is complete\n","\n","  // Launch kernel\n","  dim3 block(block_size, block_size);  // dim3 variable holds 3 dimensions\n","  dim3 grid((DSIZE+block.x-1)/block.x, (DSIZE+block.y-1)/block.y);\n","  mmul<<<grid, block>>>(d_A, d_B, d_C, DSIZE);\n","  cudaCheckErrors(\"kernel launch failure\");\n","\n","  // Cuda processing sequence step 2 is complete\n","\n","  // Copy results back to host\n","  cudaMemcpy(h_C, d_C, DSIZE*DSIZE*sizeof(float), cudaMemcpyDeviceToHost);\n","\n","  // GPU timing\n","  t2 = clock();\n","  t2sum = ((double)(t2-t1))/CLOCKS_PER_SEC;\n","  printf (\"Done. Compute took %f seconds\\n\", t2sum);\n","\n","  // Cuda processing sequence step 3 is complete\n","\n","  // Verify results\n","  cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");\n","  for (int i = 0; i < DSIZE*DSIZE; i++) if (h_C[i] != A_val*B_val*DSIZE) {printf(\"mismatch at index %d, was: %f, should be: %f\\n\", i, h_C[i], A_val*B_val*DSIZE); return -1;}\n","  printf(\"Success!\\n\");\n","  return 0;\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODge0PaYnJLr","executionInfo":{"status":"ok","timestamp":1741235965457,"user_tz":-330,"elapsed":296,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"b6575c56-aeb1-4d8b-8fdd-c28408e21608"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing matrix_mul_shared.cu\n"]}]},{"cell_type":"code","source":["!nvcc -arch=sm_75 -o matrix_mul_shared matrix_mul_shared.cu"],"metadata":{"id":"2kE52ELqnriH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./matrix_mul_shared"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_t1Hh5y2wTpO","executionInfo":{"status":"ok","timestamp":1741236044577,"user_tz":-330,"elapsed":2513,"user":{"displayName":"Sai Satish Suravazula","userId":"07818230706550831173"}},"outputId":"8b7bae91-8ba2-4775-84d2-34fad7446146"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Init took 0.571544 seconds.  Begin compute\n","Done. Compute took 1.597707 seconds\n","Success!\n"]}]},{"cell_type":"markdown","source":["Normal Matrix Multiplication took 2.103689 seconds."],"metadata":{"id":"htAGfeTbxwhy"}},{"cell_type":"code","source":[],"metadata":{"id":"JLOgknupwXi0"},"execution_count":null,"outputs":[]}]}